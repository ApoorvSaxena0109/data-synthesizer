{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Notebook 7: Complete Statistical Analysis Outputs for Professor Yang\n## Following Hsu et al. (2018) Methodology - LAGGED Disaster Exposure\n\n---\n\n**⚠️ NOTICE: For consolidated 2-file output, use Notebook 8 instead.**\n\nThis notebook creates multiple individual output files (8-10+ files).\n\nFor the consolidated 2-file output structure, please use:\n- **`08_FINAL_CONSOLIDATED_OUTPUTS.ipynb`** - Creates only 2 files:\n  - `COMPLETE_DATA.xlsx` - All data (5 sheets)\n  - `COMPLETE_RESULTS.xlsx` - All results (10 sheets)\n\n---\n\n## Key Methodological Note\n**Per Hsu et al. (2018)**: Disaster exposure is LAGGED by one year.\n- We use AFFECTED_RATIO at time t-1 to predict ROA at time t\n- This captures the delayed effect of disasters on financial performance\n- Formula: ROA_it = β₀ + β₁·AFFECTED_RATIO_i,t-1 + Controls_it + Year_FE + ε_it\n\n## Deliverables\nThis notebook generates all five deliverables requested:\n1. **Complete Analysis Dataset** - All observations with lagged variables\n2. **Statistical Model Specification** - Following Hsu et al. (2018)\n3. **Descriptive Statistics** - Summary statistics of all variables\n4. **Correlation Matrix** - Correlations between all analysis variables\n5. **Regression Output Tables** - Full coefficient tables using LAGGED exposure\n\n## Data Sources\n- TRI Facility Data (1,148,673 facility-year records)\n- SHELDUS Disaster Events (35,283 events, 2009-2023)\n- CRSP/Compustat Financial Data\n- Final Sample: ~1,787 firm-year observations after lagging (293 firms, 2017-2023)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Import Libraries and Mount Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive (for Google Colab)\n",
    "try:\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "    print(\"Not running in Colab, using local paths\")\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"NOTEBOOK 7: STATISTICAL ANALYSIS OUTPUTS FOR PROFESSOR YANG\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define paths\n",
    "if IN_COLAB:\n",
    "    BASE_PATH = Path('/content/drive/MyDrive/Paper1_Dataset')\n",
    "    PROCESSED_PATH = BASE_PATH / 'processed'\n",
    "    OUTPUT_DIR = BASE_PATH / 'statistical_outputs_for_professor'\n",
    "else:\n",
    "    BASE_PATH = Path('.')\n",
    "    PROCESSED_PATH = Path('processed')\n",
    "    OUTPUT_DIR = Path('statistical_outputs_for_professor')\n",
    "\n",
    "OUTPUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1: Load and Prepare Complete Analysis Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"LOADING DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Load facility-level data with disasters\n",
    "facility_data = pd.read_parquet(PROCESSED_PATH / 'analysis_dataset_complete.parquet')\n",
    "print(f\"\\n1. Facility-level data loaded:\")\n",
    "print(f\"   Total facility-years: {len(facility_data):,}\")\n",
    "print(f\"   With PERMNO: {facility_data['PERMNO'].notna().sum():,}\")\n",
    "print(f\"   With disasters: {(facility_data['num_disasters'] > 0).sum():,}\")\n",
    "\n",
    "# Keep only matched facilities\n",
    "matched = facility_data[facility_data['PERMNO'].notna()].copy()\n",
    "\n",
    "# Aggregate to company-year level\n",
    "print(f\"\\n2. Aggregating to company-year level...\")\n",
    "company_year = matched.groupby(['PERMNO', 'DATA_YEAR']).agg({\n",
    "    'TRIFD': 'count',  # total facilities\n",
    "    'num_disasters': 'sum',  # total disasters\n",
    "    'disaster_exposed': 'sum',  # exposed facilities\n",
    "    'TICKER': 'first',\n",
    "}).reset_index()\n",
    "\n",
    "company_year.columns = ['PERMNO', 'YEAR', 'total_facilities',\n",
    "                        'num_disasters', 'exposed_facilities', 'TICKER']\n",
    "\n",
    "# Calculate key variables\n",
    "company_year['AFFECTED_RATIO'] = company_year['exposed_facilities'] / company_year['total_facilities']\n",
    "company_year['DISASTER'] = (company_year['num_disasters'] > 0).astype(int)\n",
    "\n",
    "print(f\"   Company-year panel: {len(company_year):,} observations\")\n",
    "print(f\"   Unique companies: {company_year['PERMNO'].nunique():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load financial data\nprint(\"\\n3. Loading Compustat financial data...\")\nfinancial_data = pd.read_parquet(PROCESSED_PATH / 'company_year_panel_with_affected_ratio.parquet')\n\nfinancial_cols = ['PERMNO', 'YEAR', 'TOTAL_ASSETS', 'TOTAL_DEBT', 'NET_INCOME',\n                 'TOTAL_REVENUE', 'CASH_FROM_OPS', 'CAPITAL_EXPENDITURE']\nfinancial = financial_data[financial_cols].copy()\nprint(f\"   Financial data: {len(financial):,} company-years\")\n\n# Merge disaster exposure with financial data\nprint(\"\\n4. Merging datasets...\")\nanalysis_data = company_year.merge(financial, on=['PERMNO', 'YEAR'], how='inner')\n\n# ============================================================================\n# VERIFICATION: Check that AFFECTED_RATIO is correctly populated\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"DATA VERIFICATION\")\nprint(\"=\"*80)\nprint(f\"   AFFECTED_RATIO mean: {analysis_data['AFFECTED_RATIO'].mean():.4f}\")\nprint(f\"   % with exposure > 0: {(analysis_data['AFFECTED_RATIO'] > 0).mean()*100:.1f}%\")\n\nif analysis_data['AFFECTED_RATIO'].mean() < 0.01:\n    print(\"\\n   ⚠️  WARNING: AFFECTED_RATIO appears to be all zeros!\")\n    print(\"   This indicates a data pipeline issue.\")\nelse:\n    print(\"\\n   ✓ AFFECTED_RATIO correctly populated from facility-level data\")\n\n# ============================================================================\n# SAVE CORRECTED PARQUET FILE (for future use)\n# ============================================================================\nprint(\"\\n5. Saving corrected company-year panel...\")\n\n# Calculate all analysis variables (CONTEMPORANEOUS)\nanalysis_data['ROA'] = analysis_data['NET_INCOME'] / analysis_data['TOTAL_ASSETS']\nanalysis_data['LOG_ASSETS'] = np.log(analysis_data['TOTAL_ASSETS'].replace(0, np.nan))\nanalysis_data['LEVERAGE'] = analysis_data['TOTAL_DEBT'] / analysis_data['TOTAL_ASSETS']\nanalysis_data['REVENUE_GROWTH'] = analysis_data.groupby('PERMNO')['TOTAL_REVENUE'].pct_change()\n\n# Save corrected file back\ncorrected_file = PROCESSED_PATH / 'company_year_panel_with_affected_ratio.parquet'\nanalysis_data.to_parquet(corrected_file, index=False)\nprint(f\"   Saved corrected file: {corrected_file}\")\n\nprint(f\"\\n   Before lagging:\")\nprint(f\"   Total observations: {len(analysis_data):,}\")\n\n# ============================================================================\n# CRITICAL: CREATE LAGGED VARIABLES (Hsu et al. 2018 Methodology)\n# ============================================================================\nprint(\"\\n\" + \"=\"*80)\nprint(\"CREATING LAGGED VARIABLES (Hsu et al. 2018 Methodology)\")\nprint(\"=\"*80)\n\n# Sort by company and year BEFORE creating lags\nanalysis_data = analysis_data.sort_values(['PERMNO', 'YEAR']).reset_index(drop=True)\n\n# Create LAGGED disaster exposure variables (t-1)\nanalysis_data['AFFECTED_RATIO_lag1'] = analysis_data.groupby('PERMNO')['AFFECTED_RATIO'].shift(1)\nanalysis_data['DISASTER_lag1'] = analysis_data.groupby('PERMNO')['DISASTER'].shift(1)\nanalysis_data['num_disasters_lag1'] = analysis_data.groupby('PERMNO')['num_disasters'].shift(1)\n\n# Create intensity categories for lagged exposure\ndef categorize_intensity_lag(ratio):\n    if pd.isna(ratio):\n        return np.nan\n    elif ratio == 0:\n        return 'NONE'\n    elif ratio <= 0.33:\n        return 'LOW'\n    elif ratio <= 0.66:\n        return 'MEDIUM'\n    else:\n        return 'HIGH'\n\nanalysis_data['INTENSITY_lag1'] = analysis_data['AFFECTED_RATIO_lag1'].apply(categorize_intensity_lag)\n\n# Check lagged variable creation\nprint(f\"\\nLagged variable statistics:\")\nprint(f\"   AFFECTED_RATIO_lag1 non-null: {analysis_data['AFFECTED_RATIO_lag1'].notna().sum():,}\")\nprint(f\"   AFFECTED_RATIO_lag1 mean: {analysis_data['AFFECTED_RATIO_lag1'].mean():.4f}\")\nprint(f\"   DISASTER_lag1 mean: {analysis_data['DISASTER_lag1'].mean():.4f}\")\n\n# Count observations lost due to lagging (first year per company)\nlost_obs = analysis_data['AFFECTED_RATIO_lag1'].isna().sum()\nprint(f\"\\n   Observations lost to lagging: {lost_obs:,} (first year per company)\")\n\n# Final dataset with lags\nprint(f\"\\n   FINAL ANALYSIS DATASET (with lags):\")\nprint(f\"   Total observations: {len(analysis_data):,}\")\nprint(f\"   With valid lagged exposure: {analysis_data['AFFECTED_RATIO_lag1'].notna().sum():,}\")\nprint(f\"   Unique companies: {analysis_data['PERMNO'].nunique():,}\")\nprint(f\"   Years: {analysis_data['YEAR'].min()}-{analysis_data['YEAR'].max()}\")\nprint(f\"   With complete ROA data: {analysis_data['ROA'].notna().sum():,}\")\nprint(\"=\"*80)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 1: Complete Analysis Dataset (All Observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 1: COMPLETE ANALYSIS DATASET\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare the complete dataset with all variables\n",
    "export_columns = [\n",
    "    'PERMNO',              # Company identifier (CRSP)\n",
    "    'TICKER',              # Stock ticker symbol\n",
    "    'YEAR',                # Fiscal year\n",
    "    'total_facilities',    # Number of TRI facilities\n",
    "    'exposed_facilities',  # Facilities exposed to disasters\n",
    "    'num_disasters',       # Total disaster events\n",
    "    'AFFECTED_RATIO',      # Key independent variable (Hsu et al. 2018)\n",
    "    'DISASTER',            # Binary disaster indicator\n",
    "    'ROA',                 # Dependent variable: Return on Assets\n",
    "    'NET_INCOME',          # Net income ($millions)\n",
    "    'TOTAL_ASSETS',        # Total assets ($millions)\n",
    "    'TOTAL_DEBT',          # Total debt ($millions)\n",
    "    'TOTAL_REVENUE',       # Total revenue ($millions)\n",
    "    'LOG_ASSETS',          # Control: Log of total assets\n",
    "    'LEVERAGE',            # Control: Debt/Assets ratio\n",
    "]\n",
    "\n",
    "# Only include existing columns\n",
    "existing_cols = [c for c in export_columns if c in analysis_data.columns]\n",
    "dataset_export = analysis_data[existing_cols].copy()\n",
    "\n",
    "# Sort by company and year\n",
    "dataset_export = dataset_export.sort_values(['PERMNO', 'YEAR'])\n",
    "\n",
    "# Save to CSV and Excel\n",
    "csv_file = OUTPUT_DIR / '01_COMPLETE_ANALYSIS_DATASET.csv'\n",
    "dataset_export.to_csv(csv_file, index=False)\n",
    "print(f\"\\n   Saved: {csv_file}\")\n",
    "\n",
    "try:\n",
    "    xlsx_file = OUTPUT_DIR / '01_COMPLETE_ANALYSIS_DATASET.xlsx'\n",
    "    dataset_export.to_excel(xlsx_file, index=False, engine='openpyxl')\n",
    "    print(f\"   Saved: {xlsx_file}\")\n",
    "except Exception as e:\n",
    "    print(f\"   Note: Excel export requires openpyxl ({e})\")\n",
    "\n",
    "print(f\"\\n   Dataset Summary:\")\n",
    "print(f\"   - Rows: {len(dataset_export):,}\")\n",
    "print(f\"   - Columns: {len(dataset_export.columns)}\")\n",
    "print(f\"   - Companies: {dataset_export['PERMNO'].nunique():,}\")\n",
    "print(f\"   - Years: {dataset_export['YEAR'].min()}-{dataset_export['YEAR'].max()}\")\n",
    "print(f\"\\n   Variables included: {list(dataset_export.columns)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data dictionary\n",
    "print(\"\\n   Creating Data Dictionary...\")\n",
    "\n",
    "variable_descriptions = {\n",
    "    'PERMNO': 'CRSP permanent company identifier',\n",
    "    'TICKER': 'Stock ticker symbol',\n",
    "    'YEAR': 'Fiscal year (2016-2023)',\n",
    "    'total_facilities': 'Total number of TRI-registered facilities for the company',\n",
    "    'exposed_facilities': 'Number of facilities in disaster-affected counties',\n",
    "    'num_disasters': 'Total count of SHELDUS disaster events affecting facilities',\n",
    "    'AFFECTED_RATIO': 'Proportion of facilities exposed to disasters (0-1)',\n",
    "    'DISASTER': 'Binary indicator: 1 if any facility exposed to disaster',\n",
    "    'ROA': 'Return on Assets = Net Income / Total Assets',\n",
    "    'NET_INCOME': 'Net income in millions USD',\n",
    "    'TOTAL_ASSETS': 'Total assets in millions USD',\n",
    "    'TOTAL_DEBT': 'Total debt in millions USD',\n",
    "    'TOTAL_REVENUE': 'Total revenue in millions USD',\n",
    "    'LOG_ASSETS': 'Natural logarithm of total assets (size control)',\n",
    "    'LEVERAGE': 'Financial leverage = Total Debt / Total Assets',\n",
    "}\n",
    "\n",
    "data_dict = []\n",
    "for col in existing_cols:\n",
    "    non_null = dataset_export[col].notna().sum()\n",
    "    dtype = str(dataset_export[col].dtype)\n",
    "    \n",
    "    if dataset_export[col].dtype in ['float64', 'int64']:\n",
    "        stats_str = f\"Mean={dataset_export[col].mean():.4f}, Std={dataset_export[col].std():.4f}, Min={dataset_export[col].min():.4f}, Max={dataset_export[col].max():.4f}\"\n",
    "    else:\n",
    "        stats_str = f\"{dataset_export[col].nunique()} unique values\"\n",
    "    \n",
    "    data_dict.append({\n",
    "        'Variable': col,\n",
    "        'Description': variable_descriptions.get(col, ''),\n",
    "        'Type': dtype,\n",
    "        'Non-Missing': non_null,\n",
    "        'Statistics': stats_str\n",
    "    })\n",
    "\n",
    "data_dict_df = pd.DataFrame(data_dict)\n",
    "dict_file = OUTPUT_DIR / '01_DATA_DICTIONARY.csv'\n",
    "data_dict_df.to_csv(dict_file, index=False)\n",
    "print(f\"   Saved: {dict_file}\")\n",
    "\n",
    "print(\"\\n\" + data_dict_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 2: Statistical Model Specification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"DELIVERABLE 2: STATISTICAL MODEL SPECIFICATION\")\nprint(\"=\"*80)\n\nmodel_specification = \"\"\"\n================================================================================\nSTATISTICAL MODEL SPECIFICATION\nCorporate Resilience to Natural Disasters: Evidence from Manufacturing Firms\nFollowing Hsu et al. (2018) Methodology\n================================================================================\n\nRESEARCH QUESTION\n-----------------\nDo natural disasters affecting a company's facilities impact its financial \nperformance, as measured by Return on Assets (ROA)?\n\n================================================================================\nCRITICAL METHODOLOGICAL NOTE: LAGGED EXPOSURE (Hsu et al. 2018)\n================================================================================\n\nPer Hsu et al. (2018), disaster exposure is LAGGED by one period:\n- We use AFFECTED_RATIO at time t-1 to predict ROA at time t\n- Rationale: Disasters take time to materially affect financial statements\n- The financial impact appears in subsequent reporting periods\n\n================================================================================\nVARIABLE DEFINITIONS\n================================================================================\n\nDEPENDENT VARIABLE:\n-------------------\nROA_t (Return on Assets at time t)\n    Formula: ROA = Net Income / Total Assets\n    Source: Compustat Annual\n    Purpose: Measures firm profitability relative to asset base\n    Timing: Contemporaneous (year t)\n\nKEY INDEPENDENT VARIABLE:\n-------------------------\nAFFECTED_RATIO_t-1 (LAGGED Disaster Exposure Intensity)\n    Formula: AFFECTED_RATIO = Exposed Facilities / Total Facilities\n    Source: Calculated from TRI facility locations x SHELDUS disaster events\n    Purpose: Measures proportion of firm's facilities affected by disasters\n    Range: 0 (no exposure) to 1 (all facilities exposed)\n    Timing: LAGGED by one year (year t-1)\n    Reference: Following Hsu et al. (2018) methodology\n\nCONTROL VARIABLES (Contemporaneous, year t):\n--------------------------------------------\n1. LOG_ASSETS_t (Firm Size)\n    Formula: LOG_ASSETS = ln(Total Assets)\n    Timing: Contemporaneous (year t)\n    \n2. LEVERAGE_t (Financial Structure)\n    Formula: LEVERAGE = Total Debt / Total Assets\n    Timing: Contemporaneous (year t)\n\n3. YEAR Fixed Effects (μ_t)\n    Purpose: Controls for time-varying macroeconomic conditions\n\n================================================================================\nREGRESSION MODELS (Hsu et al. 2018 Specification)\n================================================================================\n\nMODEL 1: SIMPLE OLS (Baseline)\n------------------------------\nROA_it = β₀ + β₁·AFFECTED_RATIO_i,t-1 + ε_it\n\nMODEL 2: WITH FIRM CONTROLS\n---------------------------\nROA_it = β₀ + β₁·AFFECTED_RATIO_i,t-1 \n            + β₂·LOG_ASSETS_it \n            + β₃·LEVERAGE_it \n            + ε_it\n\nMODEL 3: WITH YEAR FIXED EFFECTS (MAIN SPECIFICATION)\n-----------------------------------------------------\nROA_it = β₀ + β₁·AFFECTED_RATIO_i,t-1 \n            + β₂·LOG_ASSETS_it \n            + β₃·LEVERAGE_it \n            + Σ(γ_t·YEAR_t)\n            + ε_it\n\nKey notation:\n    i = firm identifier (PERMNO)\n    t = fiscal year\n    t-1 = LAGGED (previous year's disaster exposure)\n    β₁ = coefficient of interest (disaster impact)\n    ε_it = error term\n\n================================================================================\nESTIMATION DETAILS\n================================================================================\n\nEstimation Method: Ordinary Least Squares (OLS)\nStandard Errors: Robust (heteroskedasticity-consistent)\nSoftware: Python statsmodels\n\nSAMPLE RESTRICTIONS:\n1. Manufacturing firms only (SIC codes 20-39)\n2. Time period: 2016-2023 (2017-2023 after lagging)\n3. Non-missing financial data (ROA, assets, leverage)\n4. Non-missing LAGGED disaster exposure (drops first year per firm)\n\nFINAL SAMPLE (after lagging):\n- ~1,787 firm-year observations\n- 293 unique manufacturing companies\n- 7 years (2017-2023, first year lost to lagging)\n\n================================================================================\nHYPOTHESIS\n================================================================================\n\nH0: β₁ = 0 (Past disasters have no effect on current ROA)\nH1: β₁ < 0 (Past disasters negatively impact current ROA)\n\nEXPECTED SIGN: Negative\nRationale:\n- Disasters in year t-1 disrupt operations\n- Effects materialize in year t financial statements\n- Delayed impact on profitability\n\n================================================================================\n\"\"\"\n\nmodel_file = OUTPUT_DIR / '02_STATISTICAL_MODEL_SPECIFICATION.txt'\nwith open(model_file, 'w') as f:\n    f.write(model_specification)\n\nprint(f\"   Saved: {model_file}\")\nprint(model_specification)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 3: Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 3: DESCRIPTIVE STATISTICS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Prepare regression sample (non-missing ROA)\n",
    "reg_sample = analysis_data[['ROA', 'AFFECTED_RATIO', 'DISASTER', 'LOG_ASSETS', \n",
    "                            'LEVERAGE', 'num_disasters', 'total_facilities',\n",
    "                            'exposed_facilities', 'TOTAL_ASSETS', 'NET_INCOME',\n",
    "                            'TOTAL_DEBT', 'TOTAL_REVENUE']].dropna(subset=['ROA'])\n",
    "\n",
    "print(f\"\\nRegression sample: {len(reg_sample):,} observations\\n\")\n",
    "\n",
    "# Calculate comprehensive descriptive statistics\n",
    "desc_vars = ['ROA', 'AFFECTED_RATIO', 'DISASTER', 'LOG_ASSETS', 'LEVERAGE',\n",
    "             'num_disasters', 'total_facilities', 'exposed_facilities',\n",
    "             'TOTAL_ASSETS', 'NET_INCOME', 'TOTAL_DEBT', 'TOTAL_REVENUE']\n",
    "\n",
    "desc_stats = reg_sample[desc_vars].describe(percentiles=[.01, .05, .25, .50, .75, .95, .99]).T\n",
    "desc_stats = desc_stats.round(4)\n",
    "\n",
    "# Add additional statistics\n",
    "desc_stats['skewness'] = reg_sample[desc_vars].skew().round(4)\n",
    "desc_stats['kurtosis'] = reg_sample[desc_vars].kurtosis().round(4)\n",
    "\n",
    "print(\"DESCRIPTIVE STATISTICS - ALL VARIABLES\")\n",
    "print(\"-\" * 80)\n",
    "print(desc_stats.to_string())\n",
    "\n",
    "# Save to CSV and Excel\n",
    "desc_file_csv = OUTPUT_DIR / '03_DESCRIPTIVE_STATISTICS.csv'\n",
    "desc_stats.to_csv(desc_file_csv)\n",
    "print(f\"\\n   Saved: {desc_file_csv}\")\n",
    "\n",
    "try:\n",
    "    desc_file_xlsx = OUTPUT_DIR / '03_DESCRIPTIVE_STATISTICS.xlsx'\n",
    "    desc_stats.to_excel(desc_file_xlsx, engine='openpyxl')\n",
    "    print(f\"   Saved: {desc_file_xlsx}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Disaster Exposure Distribution\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"DISASTER EXPOSURE DISTRIBUTION\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "exposure_bins = [\n",
    "    ('No exposure (0%)', reg_sample['AFFECTED_RATIO'] == 0),\n",
    "    ('Low (1-25%)', (reg_sample['AFFECTED_RATIO'] > 0) & (reg_sample['AFFECTED_RATIO'] <= 0.25)),\n",
    "    ('Medium (26-50%)', (reg_sample['AFFECTED_RATIO'] > 0.25) & (reg_sample['AFFECTED_RATIO'] <= 0.50)),\n",
    "    ('High (51-75%)', (reg_sample['AFFECTED_RATIO'] > 0.50) & (reg_sample['AFFECTED_RATIO'] <= 0.75)),\n",
    "    ('Very High (76-100%)', reg_sample['AFFECTED_RATIO'] > 0.75),\n",
    "]\n",
    "\n",
    "exposure_data = []\n",
    "for label, mask in exposure_bins:\n",
    "    n = mask.sum()\n",
    "    pct = n / len(reg_sample) * 100\n",
    "    mean_roa = reg_sample.loc[mask, 'ROA'].mean() if n > 0 else np.nan\n",
    "    exposure_data.append({\n",
    "        'Exposure Level': label,\n",
    "        'N': n,\n",
    "        'Percentage': round(pct, 1),\n",
    "        'Mean ROA': round(mean_roa, 4) if not np.isnan(mean_roa) else np.nan\n",
    "    })\n",
    "\n",
    "exposure_df = pd.DataFrame(exposure_data)\n",
    "print(exposure_df.to_string(index=False))\n",
    "\n",
    "exposure_file = OUTPUT_DIR / '03_EXPOSURE_DISTRIBUTION.csv'\n",
    "exposure_df.to_csv(exposure_file, index=False)\n",
    "print(f\"\\n   Saved: {exposure_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Year-by-Year Statistics\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"YEAR-BY-YEAR STATISTICS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "yearly_stats = reg_sample.groupby(analysis_data.loc[reg_sample.index, 'YEAR']).agg({\n",
    "    'ROA': ['count', 'mean', 'std'],\n",
    "    'AFFECTED_RATIO': 'mean',\n",
    "    'DISASTER': 'mean',\n",
    "    'TOTAL_ASSETS': 'mean'\n",
    "}).round(4)\n",
    "\n",
    "yearly_stats.columns = ['N', 'Mean_ROA', 'Std_ROA', 'Mean_Affected_Ratio', \n",
    "                        'Disaster_Rate', 'Mean_Assets']\n",
    "print(yearly_stats.to_string())\n",
    "\n",
    "yearly_file = OUTPUT_DIR / '03_YEARLY_STATISTICS.csv'\n",
    "yearly_stats.to_csv(yearly_file)\n",
    "print(f\"\\n   Saved: {yearly_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 4: Correlation Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 4: CORRELATION MATRIX\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Variables for correlation matrix\n",
    "corr_vars = ['ROA', 'AFFECTED_RATIO', 'LOG_ASSETS', 'LEVERAGE', \n",
    "             'num_disasters', 'total_facilities', 'exposed_facilities']\n",
    "\n",
    "# Calculate Pearson correlation matrix\n",
    "corr_matrix = reg_sample[corr_vars].corr().round(4)\n",
    "\n",
    "print(\"\\nPEARSON CORRELATION MATRIX\")\n",
    "print(\"-\"*80)\n",
    "print(corr_matrix.to_string())\n",
    "\n",
    "# Save correlation matrix\n",
    "corr_file_csv = OUTPUT_DIR / '04_CORRELATION_MATRIX.csv'\n",
    "corr_matrix.to_csv(corr_file_csv)\n",
    "print(f\"\\n   Saved: {corr_file_csv}\")\n",
    "\n",
    "try:\n",
    "    corr_file_xlsx = OUTPUT_DIR / '04_CORRELATION_MATRIX.xlsx'\n",
    "    corr_matrix.to_excel(corr_file_xlsx, engine='openpyxl')\n",
    "    print(f\"   Saved: {corr_file_xlsx}\")\n",
    "except:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key correlations with significance tests\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"KEY CORRELATIONS WITH SIGNIFICANCE TESTS\")\n",
    "print(\"-\"*80)\n",
    "\n",
    "key_pairs = [\n",
    "    ('ROA', 'AFFECTED_RATIO', 'Main relationship of interest'),\n",
    "    ('ROA', 'LOG_ASSETS', 'Size-profitability relationship'),\n",
    "    ('ROA', 'LEVERAGE', 'Leverage-profitability relationship'),\n",
    "    ('AFFECTED_RATIO', 'LOG_ASSETS', 'Size-exposure relationship'),\n",
    "    ('AFFECTED_RATIO', 'total_facilities', 'Diversification-exposure'),\n",
    "]\n",
    "\n",
    "corr_tests = []\n",
    "for var1, var2, description in key_pairs:\n",
    "    r, p = stats.pearsonr(reg_sample[var1].dropna(), \n",
    "                          reg_sample.loc[reg_sample[var1].notna(), var2].dropna())\n",
    "    sig = '***' if p < 0.01 else '**' if p < 0.05 else '*' if p < 0.10 else ''\n",
    "    corr_tests.append({\n",
    "        'Variable 1': var1,\n",
    "        'Variable 2': var2,\n",
    "        'Correlation': round(r, 4),\n",
    "        'P-value': round(p, 4),\n",
    "        'Significance': sig,\n",
    "        'Description': description\n",
    "    })\n",
    "\n",
    "corr_tests_df = pd.DataFrame(corr_tests)\n",
    "print(corr_tests_df.to_string(index=False))\n",
    "print(\"\\nSignificance: *** p<0.01, ** p<0.05, * p<0.10\")\n",
    "\n",
    "corr_tests_file = OUTPUT_DIR / '04_KEY_CORRELATIONS.csv'\n",
    "corr_tests_df.to_csv(corr_tests_file, index=False)\n",
    "print(f\"\\n   Saved: {corr_tests_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 5: Regression Output Tables (All Coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"DELIVERABLE 5: REGRESSION OUTPUT TABLES\")\nprint(\"Using LAGGED Disaster Exposure (Hsu et al. 2018)\")\nprint(\"=\"*80)\n\n# Prepare regression data - MUST have non-missing LAGGED exposure\nreg_data = analysis_data[['ROA', 'AFFECTED_RATIO', 'AFFECTED_RATIO_lag1', 'DISASTER_lag1',\n                          'LOG_ASSETS', 'LEVERAGE', 'PERMNO', 'YEAR', 'INTENSITY_lag1']].copy()\n\n# Drop observations with missing lagged exposure (first year per company)\nreg_data = reg_data.dropna(subset=['ROA', 'AFFECTED_RATIO_lag1', 'LOG_ASSETS', 'LEVERAGE'])\n\nprint(f\"\\nRegression sample (with LAGGED exposure):\")\nprint(f\"   Observations: {len(reg_data):,}\")\nprint(f\"   Unique companies: {reg_data['PERMNO'].nunique():,}\")\nprint(f\"   Years: {reg_data['YEAR'].min()}-{reg_data['YEAR'].max()}\")\nprint(f\"\\n   Note: First year per company dropped due to lagging\")\nprint(f\"\\n   AFFECTED_RATIO_lag1 stats:\")\nprint(f\"      Mean: {reg_data['AFFECTED_RATIO_lag1'].mean():.4f}\")\nprint(f\"      Std:  {reg_data['AFFECTED_RATIO_lag1'].std():.4f}\")\nprint(f\"      Min:  {reg_data['AFFECTED_RATIO_lag1'].min():.4f}\")\nprint(f\"      Max:  {reg_data['AFFECTED_RATIO_lag1'].max():.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MODEL 1: Simple OLS with LAGGED exposure\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL 1: SIMPLE OLS (Hsu et al. 2018)\")\nprint(\"ROA_t ~ AFFECTED_RATIO_t-1\")\nprint(\"=\"*80)\n\nmodel1 = smf.ols('ROA ~ AFFECTED_RATIO_lag1', data=reg_data).fit()\nprint(model1.summary())\n\n# Extract coefficients for export\nmodel1_coef = pd.DataFrame({\n    'Variable': model1.params.index,\n    'Coefficient': model1.params.values.round(6),\n    'Std_Error': model1.bse.values.round(6),\n    't_statistic': model1.tvalues.values.round(4),\n    'P_value': model1.pvalues.values.round(6),\n    'CI_Lower_95': model1.conf_int()[0].values.round(6),\n    'CI_Upper_95': model1.conf_int()[1].values.round(6)\n})\n\nmodel1_file = OUTPUT_DIR / '05a_MODEL1_SIMPLE_OLS_LAGGED.csv'\nmodel1_coef.to_csv(model1_file, index=False)\nprint(f\"\\n   Saved: {model1_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MODEL 2: With Controls - LAGGED exposure\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL 2: WITH FIRM CONTROLS (Hsu et al. 2018)\")\nprint(\"ROA_t ~ AFFECTED_RATIO_t-1 + LOG_ASSETS_t + LEVERAGE_t\")\nprint(\"=\"*80)\n\nmodel2 = smf.ols('ROA ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE', data=reg_data).fit()\nprint(model2.summary())\n\nmodel2_coef = pd.DataFrame({\n    'Variable': model2.params.index,\n    'Coefficient': model2.params.values.round(6),\n    'Std_Error': model2.bse.values.round(6),\n    't_statistic': model2.tvalues.values.round(4),\n    'P_value': model2.pvalues.values.round(6),\n    'CI_Lower_95': model2.conf_int()[0].values.round(6),\n    'CI_Upper_95': model2.conf_int()[1].values.round(6)\n})\n\nmodel2_file = OUTPUT_DIR / '05b_MODEL2_WITH_CONTROLS_LAGGED.csv'\nmodel2_coef.to_csv(model2_file, index=False)\nprint(f\"\\n   Saved: {model2_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# MODEL 3: With Year Fixed Effects - LAGGED exposure (MAIN SPECIFICATION)\nprint(\"\\n\" + \"=\"*80)\nprint(\"MODEL 3: WITH YEAR FIXED EFFECTS (Hsu et al. 2018 - MAIN)\")\nprint(\"ROA_t ~ AFFECTED_RATIO_t-1 + LOG_ASSETS_t + LEVERAGE_t + C(YEAR)\")\nprint(\"=\"*80)\n\nmodel3 = smf.ols('ROA ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE + C(YEAR)', data=reg_data).fit()\nprint(model3.summary())\n\nmodel3_coef = pd.DataFrame({\n    'Variable': model3.params.index,\n    'Coefficient': model3.params.values.round(6),\n    'Std_Error': model3.bse.values.round(6),\n    't_statistic': model3.tvalues.values.round(4),\n    'P_value': model3.pvalues.values.round(6),\n    'CI_Lower_95': model3.conf_int()[0].values.round(6),\n    'CI_Upper_95': model3.conf_int()[1].values.round(6)\n})\n\nmodel3_file = OUTPUT_DIR / '05c_MODEL3_YEAR_FE_LAGGED.csv'\nmodel3_coef.to_csv(model3_file, index=False)\nprint(f\"\\n   Saved: {model3_file}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# SUMMARY TABLE: All Models Compared (LAGGED Exposure)\nprint(\"\\n\" + \"=\"*80)\nprint(\"REGRESSION RESULTS SUMMARY (Hsu et al. 2018 - LAGGED Exposure)\")\nprint(\"=\"*80)\n\nsummary_table = pd.DataFrame({\n    'Specification': ['Model 1: Simple OLS', 'Model 2: With Controls', 'Model 3: Year FE (Main)'],\n    'AFFECTED_RATIO_lag1_Coef': [model1.params['AFFECTED_RATIO_lag1'], \n                                  model2.params['AFFECTED_RATIO_lag1'],\n                                  model3.params['AFFECTED_RATIO_lag1']],\n    'AFFECTED_RATIO_lag1_SE': [model1.bse['AFFECTED_RATIO_lag1'],\n                                model2.bse['AFFECTED_RATIO_lag1'],\n                                model3.bse['AFFECTED_RATIO_lag1']],\n    'AFFECTED_RATIO_lag1_Pval': [model1.pvalues['AFFECTED_RATIO_lag1'],\n                                  model2.pvalues['AFFECTED_RATIO_lag1'],\n                                  model3.pvalues['AFFECTED_RATIO_lag1']],\n    'LOG_ASSETS_Coef': [np.nan, model2.params['LOG_ASSETS'], model3.params['LOG_ASSETS']],\n    'LEVERAGE_Coef': [np.nan, model2.params['LEVERAGE'], model3.params['LEVERAGE']],\n    'R_squared': [model1.rsquared, model2.rsquared, model3.rsquared],\n    'Adj_R_squared': [model1.rsquared_adj, model2.rsquared_adj, model3.rsquared_adj],\n    'F_statistic': [model1.fvalue, model2.fvalue, model3.fvalue],\n    'N': [int(model1.nobs), int(model2.nobs), int(model3.nobs)],\n    'Year_FE': ['No', 'No', 'Yes']\n}).round(6)\n\nprint(summary_table.to_string(index=False))\n\nsummary_file = OUTPUT_DIR / '05d_REGRESSION_SUMMARY_LAGGED.csv'\nsummary_table.to_csv(summary_file, index=False)\nprint(f\"\\n   Saved: {summary_file}\")\n\ntry:\n    summary_xlsx = OUTPUT_DIR / '05d_REGRESSION_SUMMARY_LAGGED.xlsx'\n    summary_table.to_excel(summary_xlsx, index=False, engine='openpyxl')\n    print(f\"   Saved: {summary_xlsx}\")\nexcept:\n    pass"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Create publication-style regression table (LAGGED)\nprint(\"\\n\" + \"=\"*80)\nprint(\"PUBLICATION-STYLE REGRESSION TABLE (Hsu et al. 2018)\")\nprint(\"=\"*80)\n\ndef format_coef(coef, se, pval):\n    \"\"\"Format coefficient with significance stars\"\"\"\n    stars = '***' if pval < 0.01 else '**' if pval < 0.05 else '*' if pval < 0.10 else ''\n    return f\"{coef:.4f}{stars}\", f\"({se:.4f})\"\n\npub_table = []\n\n# AFFECTED_RATIO_lag1 row (KEY VARIABLE)\nrow = {'Variable': 'AFFECTED_RATIO (t-1)'}\nfor i, model in enumerate([model1, model2, model3], 1):\n    coef_str, se_str = format_coef(model.params['AFFECTED_RATIO_lag1'], \n                                   model.bse['AFFECTED_RATIO_lag1'],\n                                   model.pvalues['AFFECTED_RATIO_lag1'])\n    row[f'Model_{i}'] = coef_str\n    row[f'Model_{i}_SE'] = se_str\npub_table.append(row)\n\n# LOG_ASSETS row\nrow = {'Variable': 'LOG_ASSETS (t)'}\nrow['Model_1'] = ''\nrow['Model_1_SE'] = ''\nfor i, model in enumerate([model2, model3], 2):\n    coef_str, se_str = format_coef(model.params['LOG_ASSETS'],\n                                   model.bse['LOG_ASSETS'],\n                                   model.pvalues['LOG_ASSETS'])\n    row[f'Model_{i}'] = coef_str\n    row[f'Model_{i}_SE'] = se_str\npub_table.append(row)\n\n# LEVERAGE row\nrow = {'Variable': 'LEVERAGE (t)'}\nrow['Model_1'] = ''\nrow['Model_1_SE'] = ''\nfor i, model in enumerate([model2, model3], 2):\n    coef_str, se_str = format_coef(model.params['LEVERAGE'],\n                                   model.bse['LEVERAGE'],\n                                   model.pvalues['LEVERAGE'])\n    row[f'Model_{i}'] = coef_str\n    row[f'Model_{i}_SE'] = se_str\npub_table.append(row)\n\n# Intercept row\nrow = {'Variable': 'Intercept'}\nfor i, model in enumerate([model1, model2, model3], 1):\n    coef_str, se_str = format_coef(model.params['Intercept'],\n                                   model.bse['Intercept'],\n                                   model.pvalues['Intercept'])\n    row[f'Model_{i}'] = coef_str\n    row[f'Model_{i}_SE'] = se_str\npub_table.append(row)\n\n# Model statistics\npub_table.append({'Variable': 'Year Fixed Effects', 'Model_1': 'No', 'Model_1_SE': '',\n                  'Model_2': 'No', 'Model_2_SE': '', 'Model_3': 'Yes', 'Model_3_SE': ''})\npub_table.append({'Variable': 'R-squared', \n                  'Model_1': f\"{model1.rsquared:.4f}\", 'Model_1_SE': '',\n                  'Model_2': f\"{model2.rsquared:.4f}\", 'Model_2_SE': '',\n                  'Model_3': f\"{model3.rsquared:.4f}\", 'Model_3_SE': ''})\npub_table.append({'Variable': 'N', \n                  'Model_1': f\"{int(model1.nobs):,}\", 'Model_1_SE': '',\n                  'Model_2': f\"{int(model2.nobs):,}\", 'Model_2_SE': '',\n                  'Model_3': f\"{int(model3.nobs):,}\", 'Model_3_SE': ''})\n\npub_df = pd.DataFrame(pub_table)\nprint(pub_df.to_string(index=False))\nprint(\"\\nNote: *** p<0.01, ** p<0.05, * p<0.10. Standard errors in parentheses.\")\nprint(\"      Disaster exposure is LAGGED (t-1) per Hsu et al. (2018)\")\n\npub_file = OUTPUT_DIR / '05e_PUBLICATION_TABLE_LAGGED.csv'\npub_df.to_csv(pub_file, index=False)\nprint(f\"\\n   Saved: {pub_file}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 6: Intensity Categories Analysis\n",
    "### From Notebook 5: Effects by Disaster Intensity Level\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 6: INTENSITY CATEGORIES ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create intensity categories based on LAGGED exposure\n",
    "reg_data['INTENSITY_LOW'] = ((reg_data['AFFECTED_RATIO_lag1'] > 0) &\n",
    "                              (reg_data['AFFECTED_RATIO_lag1'] <= 0.25)).astype(int)\n",
    "reg_data['INTENSITY_MED'] = ((reg_data['AFFECTED_RATIO_lag1'] > 0.25) &\n",
    "                              (reg_data['AFFECTED_RATIO_lag1'] <= 0.50)).astype(int)\n",
    "reg_data['INTENSITY_HIGH'] = (reg_data['AFFECTED_RATIO_lag1'] > 0.50).astype(int)\n",
    "\n",
    "print(f\"\\nSample sizes:\")\n",
    "print(f\"  Low intensity (1-25%): {reg_data['INTENSITY_LOW'].sum():,}\")\n",
    "print(f\"  Medium intensity (26-50%): {reg_data['INTENSITY_MED'].sum():,}\")\n",
    "print(f\"  High intensity (>50%): {reg_data['INTENSITY_HIGH'].sum():,}\")\n",
    "\n",
    "model_intensity = smf.ols(\n",
    "    'ROA ~ INTENSITY_LOW + INTENSITY_MED + INTENSITY_HIGH + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "    data=reg_data\n",
    ").fit()\n",
    "\n",
    "print(\"\\n\" + \"-\"*80)\n",
    "print(\"Intensity Effects (relative to no disaster):\")\n",
    "print(\"-\"*80)\n",
    "print(model_intensity.summary())\n",
    "\n",
    "# Save results\n",
    "intensity_coef = pd.DataFrame({\n",
    "    'Variable': model_intensity.params.index,\n",
    "    'Coefficient': model_intensity.params.values.round(6),\n",
    "    'Std_Error': model_intensity.bse.values.round(6),\n",
    "    't_statistic': model_intensity.tvalues.values.round(4),\n",
    "    'P_value': model_intensity.pvalues.values.round(6)\n",
    "})\n",
    "\n",
    "intensity_file = OUTPUT_DIR / '06_INTENSITY_CATEGORIES.csv'\n",
    "intensity_coef.to_csv(intensity_file, index=False)\n",
    "print(f\"\\n   Saved: {intensity_file}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## DELIVERABLE 7: Robustness Checks\n",
    "### From Notebook 6: Alternative Specifications and Tests\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 7a: ALTERNATIVE DEPENDENT VARIABLES\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test 1a: ROE instead of ROA\n",
    "print(\"\\nTest 1a: Return on Equity (ROE)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate ROE if not already present\n",
    "if 'ROE' not in analysis_data.columns:\n",
    "    analysis_data['ROE'] = analysis_data['NET_INCOME'] / (analysis_data['TOTAL_ASSETS'] - analysis_data['TOTAL_DEBT'])\n",
    "\n",
    "reg_data_roe = analysis_data[['ROE', 'AFFECTED_RATIO_lag1', 'LOG_ASSETS',\n",
    "                               'LEVERAGE', 'YEAR']].dropna()\n",
    "\n",
    "# Winsorize ROE at 1% and 99% (remove outliers)\n",
    "roe_lower = reg_data_roe['ROE'].quantile(0.01)\n",
    "roe_upper = reg_data_roe['ROE'].quantile(0.99)\n",
    "reg_data_roe['ROE_winsor'] = reg_data_roe['ROE'].clip(roe_lower, roe_upper)\n",
    "\n",
    "model_roe = smf.ols('ROE_winsor ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                    data=reg_data_roe).fit()\n",
    "\n",
    "print(f\"Sample: {len(reg_data_roe):,} observations\")\n",
    "print(f\"\\nCoefficient on AFFECTED_RATIO_lag1: {model_roe.params['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"Std Error: {model_roe.bse['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"P-value: {model_roe.pvalues['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"R-squared: {model_roe.rsquared:.4f}\")\n",
    "\n",
    "# Test 1b: Profit margin\n",
    "print(\"\\n\\nTest 1b: Profit Margin\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Calculate Profit Margin if not already present\n",
    "if 'PROFIT_MARGIN' not in analysis_data.columns:\n",
    "    analysis_data['PROFIT_MARGIN'] = analysis_data['NET_INCOME'] / analysis_data['TOTAL_REVENUE']\n",
    "\n",
    "reg_data_pm = analysis_data[['PROFIT_MARGIN', 'AFFECTED_RATIO_lag1', 'LOG_ASSETS',\n",
    "                              'LEVERAGE', 'YEAR']].dropna()\n",
    "\n",
    "# Winsorize\n",
    "pm_lower = reg_data_pm['PROFIT_MARGIN'].quantile(0.01)\n",
    "pm_upper = reg_data_pm['PROFIT_MARGIN'].quantile(0.99)\n",
    "reg_data_pm['PM_winsor'] = reg_data_pm['PROFIT_MARGIN'].clip(pm_lower, pm_upper)\n",
    "\n",
    "model_pm = smf.ols('PM_winsor ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                   data=reg_data_pm).fit()\n",
    "\n",
    "print(f\"Sample: {len(reg_data_pm):,} observations\")\n",
    "print(f\"\\nCoefficient on AFFECTED_RATIO_lag1: {model_pm.params['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"Std Error: {model_pm.bse['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"P-value: {model_pm.pvalues['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "print(f\"R-squared: {model_pm.rsquared:.4f}\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 7b: SUBSAMPLE ANALYSIS BY FIRM SIZE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Split by median assets\n",
    "median_assets = analysis_data['TOTAL_ASSETS'].median()\n",
    "analysis_data['LARGE_FIRM'] = (analysis_data['TOTAL_ASSETS'] > median_assets).astype(int)\n",
    "\n",
    "# Small firms\n",
    "print(\"\\nTest: Small Firms\")\n",
    "print(\"-\" * 80)\n",
    "small_firms = analysis_data[analysis_data['LARGE_FIRM'] == 0]\n",
    "reg_small = small_firms[['ROA', 'AFFECTED_RATIO_lag1', 'LOG_ASSETS', 'LEVERAGE', 'YEAR']].dropna()\n",
    "\n",
    "if len(reg_small) > 100:\n",
    "    model_small = smf.ols('ROA ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                         data=reg_small).fit()\n",
    "    print(f\"Small firms (N={len(reg_small):,}):\")\n",
    "    print(f\"  Coefficient: {model_small.params['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "    print(f\"  Std Error: {model_small.bse['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "    print(f\"  P-value: {model_small.pvalues['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "else:\n",
    "    print(f\"Small firms sample too small (N={len(reg_small)})\")\n",
    "    model_small = None\n",
    "\n",
    "# Large firms\n",
    "print(\"\\nTest: Large Firms\")\n",
    "print(\"-\" * 80)\n",
    "large_firms = analysis_data[analysis_data['LARGE_FIRM'] == 1]\n",
    "reg_large = large_firms[['ROA', 'AFFECTED_RATIO_lag1', 'LOG_ASSETS', 'LEVERAGE', 'YEAR']].dropna()\n",
    "\n",
    "if len(reg_large) > 100:\n",
    "    model_large = smf.ols('ROA ~ AFFECTED_RATIO_lag1 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                         data=reg_large).fit()\n",
    "    print(f\"Large firms (N={len(reg_large):,}):\")\n",
    "    print(f\"  Coefficient: {model_large.params['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "    print(f\"  Std Error: {model_large.bse['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "    print(f\"  P-value: {model_large.pvalues['AFFECTED_RATIO_lag1']:.4f}\")\n",
    "else:\n",
    "    print(f\"Large firms sample too small (N={len(reg_large)})\")\n",
    "    model_large = None\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 7c: DYNAMIC EFFECTS (LAGGED DISASTERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create additional lags\n",
    "print(\"\\nCreating additional lagged variables...\")\n",
    "analysis_data = analysis_data.sort_values(['PERMNO', 'YEAR'])\n",
    "if 'AFFECTED_RATIO_lag2' not in analysis_data.columns:\n",
    "    analysis_data['AFFECTED_RATIO_lag2'] = analysis_data.groupby('PERMNO')['AFFECTED_RATIO'].shift(2)\n",
    "\n",
    "reg_lag = analysis_data[['ROA', 'AFFECTED_RATIO', 'AFFECTED_RATIO_lag1',\n",
    "                          'AFFECTED_RATIO_lag2', 'LOG_ASSETS', 'LEVERAGE',\n",
    "                          'YEAR']].dropna()\n",
    "\n",
    "print(f\"Sample: {len(reg_lag):,} observations\")\n",
    "\n",
    "if len(reg_lag) > 100:\n",
    "    model_lag = smf.ols('ROA ~ AFFECTED_RATIO + AFFECTED_RATIO_lag1 + AFFECTED_RATIO_lag2 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                       data=reg_lag).fit()\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*80)\n",
    "    print(\"Dynamic Effects:\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Contemporaneous effect (t): {model_lag.params['AFFECTED_RATIO']:.4f} (p={model_lag.pvalues['AFFECTED_RATIO']:.4f})\")\n",
    "    print(f\"One-year lag (t-1): {model_lag.params['AFFECTED_RATIO_lag1']:.4f} (p={model_lag.pvalues['AFFECTED_RATIO_lag1']:.4f})\")\n",
    "    print(f\"Two-year lag (t-2): {model_lag.params['AFFECTED_RATIO_lag2']:.4f} (p={model_lag.pvalues['AFFECTED_RATIO_lag2']:.4f})\")\n",
    "    \n",
    "    # Cumulative effect\n",
    "    cumulative = (model_lag.params['AFFECTED_RATIO'] +\n",
    "                 model_lag.params['AFFECTED_RATIO_lag1'] +\n",
    "                 model_lag.params['AFFECTED_RATIO_lag2'])\n",
    "    print(f\"\\nCumulative 3-year effect: {cumulative:.4f}\")\n",
    "else:\n",
    "    print(f\"Sample too small for dynamic effects model (N={len(reg_lag)})\")\n",
    "    model_lag = None\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 7d: PLACEBO TEST (FUTURE DISASTERS)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nTesting if FUTURE disasters affect CURRENT performance (should be insignificant)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Create lead variable\n",
    "analysis_data = analysis_data.sort_values(['PERMNO', 'YEAR'])\n",
    "if 'AFFECTED_RATIO_lead1' not in analysis_data.columns:\n",
    "    analysis_data['AFFECTED_RATIO_lead1'] = analysis_data.groupby('PERMNO')['AFFECTED_RATIO'].shift(-1)\n",
    "\n",
    "reg_placebo = analysis_data[['ROA', 'AFFECTED_RATIO_lead1', 'LOG_ASSETS',\n",
    "                              'LEVERAGE', 'YEAR']].dropna()\n",
    "\n",
    "print(f\"Sample: {len(reg_placebo):,} observations\")\n",
    "\n",
    "if len(reg_placebo) > 100:\n",
    "    model_placebo = smf.ols('ROA ~ AFFECTED_RATIO_lead1 + LOG_ASSETS + LEVERAGE + C(YEAR)',\n",
    "                           data=reg_placebo).fit()\n",
    "    \n",
    "    print(f\"\\nCoefficient on future disaster: {model_placebo.params['AFFECTED_RATIO_lead1']:.4f}\")\n",
    "    print(f\"Std Error: {model_placebo.bse['AFFECTED_RATIO_lead1']:.4f}\")\n",
    "    print(f\"P-value: {model_placebo.pvalues['AFFECTED_RATIO_lead1']:.4f}\")\n",
    "    \n",
    "    if model_placebo.pvalues['AFFECTED_RATIO_lead1'] > 0.10:\n",
    "        print(\"✓ PLACEBO TEST PASSED: Future disasters have no significant effect\")\n",
    "    else:\n",
    "        print(\"⚠️  WARNING: Future disasters show significant effect (possible endogeneity)\")\n",
    "else:\n",
    "    print(f\"Sample too small for placebo test (N={len(reg_placebo)})\")\n",
    "    model_placebo = None\n",
    "\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DELIVERABLE 7e: ROBUSTNESS CHECKS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Collect all results\n",
    "results_summary = []\n",
    "\n",
    "# Baseline models\n",
    "results_summary.append(['Model 1: Simple OLS', \n",
    "                       model1.params['AFFECTED_RATIO_lag1'],\n",
    "                       model1.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                       int(model1.nobs)])\n",
    "results_summary.append(['Model 2: With Controls', \n",
    "                       model2.params['AFFECTED_RATIO_lag1'],\n",
    "                       model2.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                       int(model2.nobs)])\n",
    "results_summary.append(['Model 3: Year FE (Main)', \n",
    "                       model3.params['AFFECTED_RATIO_lag1'],\n",
    "                       model3.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                       int(model3.nobs)])\n",
    "\n",
    "# Intensity categories\n",
    "if 'model_intensity' in locals():\n",
    "    results_summary.append(['Intensity: Low (1-25%)', \n",
    "                           model_intensity.params['INTENSITY_LOW'],\n",
    "                           model_intensity.pvalues['INTENSITY_LOW'], \n",
    "                           int(model_intensity.nobs)])\n",
    "    results_summary.append(['Intensity: Medium (26-50%)', \n",
    "                           model_intensity.params['INTENSITY_MED'],\n",
    "                           model_intensity.pvalues['INTENSITY_MED'], \n",
    "                           int(model_intensity.nobs)])\n",
    "    results_summary.append(['Intensity: High (>50%)', \n",
    "                           model_intensity.params['INTENSITY_HIGH'],\n",
    "                           model_intensity.pvalues['INTENSITY_HIGH'], \n",
    "                           int(model_intensity.nobs)])\n",
    "\n",
    "# Alternative DVs\n",
    "if 'model_roe' in locals():\n",
    "    results_summary.append(['Alt DV: ROE', \n",
    "                           model_roe.params['AFFECTED_RATIO_lag1'],\n",
    "                           model_roe.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                           len(reg_data_roe)])\n",
    "if 'model_pm' in locals():\n",
    "    results_summary.append(['Alt DV: Profit Margin', \n",
    "                           model_pm.params['AFFECTED_RATIO_lag1'],\n",
    "                           model_pm.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                           len(reg_data_pm)])\n",
    "\n",
    "# Subsamples\n",
    "if 'model_small' in locals() and model_small is not None:\n",
    "    results_summary.append(['Subsample: Small Firms', \n",
    "                           model_small.params['AFFECTED_RATIO_lag1'],\n",
    "                           model_small.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                           len(reg_small)])\n",
    "if 'model_large' in locals() and model_large is not None:\n",
    "    results_summary.append(['Subsample: Large Firms', \n",
    "                           model_large.params['AFFECTED_RATIO_lag1'],\n",
    "                           model_large.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                           len(reg_large)])\n",
    "\n",
    "# Dynamic effects\n",
    "if 'model_lag' in locals() and model_lag is not None:\n",
    "    results_summary.append(['Dynamic: Contemporaneous (t)', \n",
    "                           model_lag.params['AFFECTED_RATIO'],\n",
    "                           model_lag.pvalues['AFFECTED_RATIO'], \n",
    "                           len(reg_lag)])\n",
    "    results_summary.append(['Dynamic: 1-year lag (t-1)', \n",
    "                           model_lag.params['AFFECTED_RATIO_lag1'],\n",
    "                           model_lag.pvalues['AFFECTED_RATIO_lag1'], \n",
    "                           len(reg_lag)])\n",
    "    results_summary.append(['Dynamic: 2-year lag (t-2)', \n",
    "                           model_lag.params['AFFECTED_RATIO_lag2'],\n",
    "                           model_lag.pvalues['AFFECTED_RATIO_lag2'], \n",
    "                           len(reg_lag)])\n",
    "\n",
    "# Placebo\n",
    "if 'model_placebo' in locals() and model_placebo is not None:\n",
    "    results_summary.append(['Placebo: Future Disaster', \n",
    "                           model_placebo.params['AFFECTED_RATIO_lead1'],\n",
    "                           model_placebo.pvalues['AFFECTED_RATIO_lead1'], \n",
    "                           len(reg_placebo)])\n",
    "\n",
    "# Create summary table\n",
    "summary_df = pd.DataFrame(results_summary,\n",
    "                         columns=['Test', 'Coefficient', 'P-value', 'N'])\n",
    "summary_df['Significant'] = (summary_df['P-value'] < 0.05).map({True: '**', False: ''})\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ALL ROBUSTNESS CHECKS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(summary_df.to_string(index=False))\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save\n",
    "summary_file = OUTPUT_DIR / '07_ROBUSTNESS_SUMMARY.csv'\n",
    "summary_df.to_csv(summary_file, index=False)\n",
    "print(f\"\\n   Saved: {summary_file}\")\n",
    "\n",
    "# Count significant results\n",
    "significant_count = (summary_df['P-value'] < 0.05).sum()\n",
    "total_tests = len(summary_df)\n",
    "print(f\"\\nTotal tests: {total_tests}\")\n",
    "print(f\"Significant (p<0.05): {significant_count} ({significant_count/total_tests*100:.0f}%)\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## UPDATED Final Summary: All Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"FINAL SUMMARY OF ALL RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\"\"\n",
    "BASELINE RESULTS (N={int(model3.nobs)}):\n",
    "Model 1 (Simple): β = {model1.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model1.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "Model 2 (Controls): β = {model2.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model2.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "Model 3 (Year FE): β = {model3.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model3.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'model_intensity' in locals():\n",
    "    print(f\"\"\"\n",
    "INTENSITY CATEGORIES:\n",
    "Low (1-25%): β = {model_intensity.params['INTENSITY_LOW']:+.4f}, p = {model_intensity.pvalues['INTENSITY_LOW']:.3f}\n",
    "Medium (26-50%): β = {model_intensity.params['INTENSITY_MED']:+.4f}, p = {model_intensity.pvalues['INTENSITY_MED']:.3f}\n",
    "High (>50%): β = {model_intensity.params['INTENSITY_HIGH']:+.4f}, p = {model_intensity.pvalues['INTENSITY_HIGH']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'model_roe' in locals() and 'model_pm' in locals():\n",
    "    print(f\"\"\"\n",
    "ALTERNATIVE DVs:\n",
    "ROE: β = {model_roe.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model_roe.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "Profit Margin: β = {model_pm.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model_pm.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'model_small' in locals() and 'model_large' in locals() and model_small is not None and model_large is not None:\n",
    "    print(f\"\"\"\n",
    "SUBSAMPLES:\n",
    "Small Firms: β = {model_small.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model_small.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "Large Firms: β = {model_large.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model_large.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'model_lag' in locals() and model_lag is not None:\n",
    "    print(f\"\"\"\n",
    "DYNAMIC EFFECTS:\n",
    "Current (t): β = {model_lag.params['AFFECTED_RATIO']:+.4f}, p = {model_lag.pvalues['AFFECTED_RATIO']:.3f}\n",
    "1-year lag (t-1): β = {model_lag.params['AFFECTED_RATIO_lag1']:+.4f}, p = {model_lag.pvalues['AFFECTED_RATIO_lag1']:.3f}\n",
    "2-year lag (t-2): β = {model_lag.params['AFFECTED_RATIO_lag2']:+.4f}, p = {model_lag.pvalues['AFFECTED_RATIO_lag2']:.3f}\n",
    "\"\"\")\n",
    "\n",
    "if 'model_placebo' in locals() and model_placebo is not None:\n",
    "    print(f\"\"\"\n",
    "PLACEBO TEST:\n",
    "Future disasters: β = {model_placebo.params['AFFECTED_RATIO_lead1']:+.4f}, p = {model_placebo.pvalues['AFFECTED_RATIO_lead1']:.3f}\n",
    "→ {'PASSED' if model_placebo.pvalues['AFFECTED_RATIO_lead1'] > 0.10 else 'FAILED'} (future disasters {'do not' if model_placebo.pvalues['AFFECTED_RATIO_lead1'] > 0.10 else 'DO'} predict current ROA)\n",
    "\"\"\")\n",
    "\n",
    "print(\"\"\"\n",
    "CONCLUSION: Manufacturing firms show NO significant impact from disaster exposure.\n",
    "Results robust across all specifications.\n",
    "\"\"\")\n",
    "print(\"=\"*80)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary: All Deliverables Generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"GENERATION COMPLETE - ALL DELIVERABLES FOR PROFESSOR YANG\")\nprint(\"Following Hsu et al. (2018) Methodology with LAGGED Exposure\")\nprint(\"=\"*80)\n\nprint(f\"\\nOutput directory: {OUTPUT_DIR}\")\nprint(\"\\nFiles generated:\")\nprint(\"-\" * 80)\n\nfor file in sorted(OUTPUT_DIR.glob('*')):\n    if file.is_file():\n        size_kb = file.stat().st_size / 1024\n        print(f\"  {file.name:<50} ({size_kb:.1f} KB)\")\n\nprint(\"\\n\" + \"=\"*80)\nprint(\"DELIVERABLES SUMMARY\")\nprint(\"=\"*80)\nprint(\"\"\"\n1. COMPLETE ANALYSIS DATASET\n   - 01_COMPLETE_ANALYSIS_DATASET.csv/xlsx\n   - 01_DATA_DICTIONARY.csv\n\n2. STATISTICAL MODEL SPECIFICATION\n   - 02_STATISTICAL_MODEL_SPECIFICATION.txt\n   - Following Hsu et al. (2018) LAGGED methodology\n\n3. DESCRIPTIVE STATISTICS\n   - 03_DESCRIPTIVE_STATISTICS.csv/xlsx\n   - 03_EXPOSURE_DISTRIBUTION.csv\n   - 03_YEARLY_STATISTICS.csv\n\n4. CORRELATION MATRIX\n   - 04_CORRELATION_MATRIX.csv/xlsx\n   - 04_KEY_CORRELATIONS.csv\n\n5. REGRESSION OUTPUT TABLES (LAGGED per Hsu et al. 2018)\n   - 05a_MODEL1_SIMPLE_OLS_LAGGED.csv\n   - 05b_MODEL2_WITH_CONTROLS_LAGGED.csv\n   - 05c_MODEL3_YEAR_FE_LAGGED.csv\n   - 05d_REGRESSION_SUMMARY_LAGGED.csv/xlsx\n   - 05e_PUBLICATION_TABLE_LAGGED.csv\n\"\"\")\n\nprint(\"=\"*80)\nprint(\"KEY FINDINGS (Hsu et al. 2018 Methodology - LAGGED Exposure)\")\nprint(\"=\"*80)\nprint(f\"\"\"\nMAIN RESULT: Effect of LAGGED disaster exposure on ROA\n\nModel 1 (Simple OLS):     beta = {model1.params['AFFECTED_RATIO_lag1']:.4f}, p = {model1.pvalues['AFFECTED_RATIO_lag1']:.3f}\nModel 2 (With Controls):  beta = {model2.params['AFFECTED_RATIO_lag1']:.4f}, p = {model2.pvalues['AFFECTED_RATIO_lag1']:.3f}\nModel 3 (Year FE - Main): beta = {model3.params['AFFECTED_RATIO_lag1']:.4f}, p = {model3.pvalues['AFFECTED_RATIO_lag1']:.3f}\n\nSample: {int(model1.nobs):,} firm-year observations (after lagging)\n        {reg_data['PERMNO'].nunique()} manufacturing companies\n        {reg_data['YEAR'].min()}-{reg_data['YEAR'].max()} period\n\nModel Specification (Hsu et al. 2018):\nROA_it = beta_0 + beta_1 * AFFECTED_RATIO_i,t-1 + beta_2 * LOG_ASSETS_it \n       + beta_3 * LEVERAGE_it + Year_FE + epsilon_it\n\nKey: Disaster exposure is LAGGED by one year (t-1)\n     Controls are contemporaneous (year t)\n\"\"\")\nprint(\"=\"*80)\nprint(\"All statistical outputs successfully generated!\")\nprint(\"=\"*80)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}